{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment analysis with deep learning model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNU/nEQBGVEqkfJ+DXvYhaQ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Download data from Kaggle"
      ],
      "metadata": {
        "id": "aqQ32Ne9SP4V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "pYPHomDaSLlN",
        "outputId": "3fe404bc-7a01-4cfd-aafb-1b7221396805"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f601856f-854d-4530-ac1d-c6e1024b5850\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f601856f-854d-4530-ac1d-c6e1024b5850\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle (3).json\n",
            "kaggle.json\n",
            "vietnamese-ecommerce-review.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.upload() #upload kaggle.json\n",
        "\n",
        "!pip install -q kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!ls ~/.kaggle\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "\n",
        "!kaggle datasets download -d hienbm/vietnamese-ecommerce-review"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download library and preparing dataset"
      ],
      "metadata": {
        "id": "Ak9tYJQRSVvY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install emot\n",
        "!pip install emoji\n",
        "# !pip install vncorenlp\n",
        "# !mkdir -p vncorenlp/models/wordsegmenter\n",
        "# !wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/VnCoreNLP-1.1.1.jar\n",
        "# !wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/vi-vocab\n",
        "# !wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/wordsegmenter.rdr\n",
        "# !mv VnCoreNLP-1.1.1.jar vncorenlp/ \n",
        "# !mv vi-vocab vncorenlp/models/wordsegmenter/\n",
        "# !mv wordsegmenter.rdr vncorenlp/models/wordsegmenter/\n",
        "!pip install pyvi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLYfQuakSPMB",
        "outputId": "8875e4ad-32a3-44e7-f0c2-dcb610bfeec7"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: emot in /usr/local/lib/python3.7/dist-packages (3.1)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.7/dist-packages (1.7.0)\n",
            "Requirement already satisfied: pyvi in /usr/local/lib/python3.7/dist-packages (0.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pyvi) (1.0.2)\n",
            "Requirement already satisfied: sklearn-crfsuite in /usr/local/lib/python3.7/dist-packages (from pyvi) (0.3.6)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyvi) (1.21.5)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyvi) (1.4.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyvi) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyvi) (1.1.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite->pyvi) (0.8.9)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite->pyvi) (4.63.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite->pyvi) (1.15.0)\n",
            "Requirement already satisfied: python-crfsuite>=0.8.3 in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite->pyvi) (0.9.8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "typo_mistake = pd.read_table('https://raw.githubusercontent.com/HienBM/text_normalization/main/correct_typo_mistake.txt', delimiter='\\t')\n",
        "dict_typo_mistake = dict(zip(typo_mistake['word'], typo_mistake['typo_mistake']))\n",
        "\n",
        "vietnamese_word_dict = pd.read_table(\"https://raw.githubusercontent.com/undertheseanlp/dictionary/hongocduc/data/Viet74K.txt\")\n",
        "\n",
        "en2vi_path = pd.read_table('https://raw.githubusercontent.com/HienBM/text_normalization/main/top_500_adjective_eng.txt', delimiter='\\t')\n",
        "en2vi_dict = dict(zip(en2vi_path['Adj'], en2vi_path['Mean']))\n",
        "\n",
        "font_correct_path = pd.read_table('https://raw.githubusercontent.com/HienBM/text_normalization/main/correct_char.txt', delimiter='\\t')\n",
        "font_correct_dict = dict(zip(font_correct_path['char'], font_correct_path['correct_char']))"
      ],
      "metadata": {
        "id": "Z7xnXVzRQQdD"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/vietnamese-ecommerce-review.zip', compression='zip')\n",
        "df = df.dropna().drop_duplicates()\n",
        "df = df[(df['score'] > 0) & (len(df['content']) > 0)]\n",
        "df = df.sample(frac=1).reset_index(drop=True)\n",
        "df = df[0:100000]"
      ],
      "metadata": {
        "id": "HLYOEhhoSmNy"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing data"
      ],
      "metadata": {
        "id": "ld4bCvLySzDk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unicodedata import digit\n",
        "import re\n",
        "import string\n",
        "import emoji\n",
        "import functools\n",
        "import operator\n",
        "# from vncorenlp import VnCoreNLP\n",
        "from pyvi import ViTokenizer, ViPosTagger, ViUtils\n",
        "from emot.emo_unicode import UNICODE_EMOJI, EMOTICONS_EMO\n",
        "from nltk.tokenize import word_tokenize\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "def remove_duplicate_emoji(orig_str):\n",
        "    prev_emoji = None\n",
        "    remove_duplicate_emoji = []\n",
        "    for c in orig_str:\n",
        "        if c in UNICODE_EMOJI:\n",
        "            if prev_emoji == c:\n",
        "                continue\n",
        "            prev_emoji = c\n",
        "        remove_duplicate_emoji.append(c)\n",
        "    return \"\".join(remove_duplicate_emoji)\n",
        "\n",
        "\n",
        "def split_emoji(string):\n",
        "    em_split_emoji = emoji.get_emoji_regexp().split(string)\n",
        "    em_split_whitespace = [substr.split() for substr in em_split_emoji]\n",
        "    em_split = functools.reduce(operator.concat, em_split_whitespace)\n",
        "    em_split = ' '.join(em_split)\n",
        "    return em_split\n",
        "\n",
        "\n",
        "def preprocessing_vietnamese(text):\n",
        "\n",
        "    intab_l = list(\"ạảãàáâậầấẩẫăắằặẳẵóòọõỏôộổỗồốơờớợởỡéèẻẹẽêếềệểễúùụủũưựữửừứíìịỉĩýỳỷỵỹđ\")\n",
        "    ascii_lowercase = list(string.ascii_lowercase)\n",
        "    digits = list(string.digits)\n",
        "    punctuation = list(string.punctuation)\n",
        "    whitespace = list(' ')\n",
        "    digits_dict = {'0':'không','1':'một','2':'hai','3':'ba','4':'bốn','5':'năm','6':'sáu','7':'bảy','8':'tám','9':'chín'}\n",
        "\n",
        "    emoticon = [x for x in EMOTICONS_EMO]\n",
        "    emoji = [x for x in UNICODE_EMOJI]\n",
        "\n",
        "    # dấu câu\n",
        "    accept_char = intab_l + emoji + ascii_lowercase + whitespace\n",
        "\n",
        "    # remove hastag, mention, url\n",
        "    text = re.sub(r'<[^<]+?>','',text)\n",
        "    text = re.sub(\"@[A-Za-z0-9_]+\",\"\", text)\n",
        "    text = re.sub(\"#[A-Za-z0-9_]+\",\"\", text)\n",
        "    text = re.sub('http[s]?://\\S+', '', text)\n",
        "\n",
        "    # convert text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # # convert digits to text\n",
        "    # for k, v in digits_dict.items():\n",
        "    #     value = ' '+v+' '\n",
        "    #     text = text.replace(k, value)\n",
        "\n",
        "    # map correct font\n",
        "    for k,v in font_correct_dict.items():\n",
        "        text = text.replace(k,v)\n",
        "\n",
        "    # remove characters if not in vietnamese alphabet\n",
        "    text = [letter if letter in accept_char else ' ' for letter in text]\n",
        "    text = ''.join(text)\n",
        "\n",
        "    # # mapping dấu câu thành dấu chấm câu\n",
        "    # text = re.sub(r'[,!?;-]+', '.', text)\n",
        "    # text = text.replace('.','. ')\n",
        "    # text = text.replace(' .','.')\n",
        "    # text = re.sub(r'([.])\\1+', r'\\1', text)\n",
        "\n",
        "    # chuẩn hóa các từ elongated\n",
        "    text = re.sub(r'([a-z]+?)\\1+', r'\\1\\1', text)\n",
        "\n",
        "    # loại bỏ các từ lặp lại liên tiếp\n",
        "    text = re.sub(r'\\b(\\w+)( \\1\\b)+', r'\\1', text)\n",
        "\n",
        "    # loại bỏ duplicate emoji\n",
        "    text = split_emoji(text)\n",
        "    text = remove_duplicate_emoji(text)\n",
        "\n",
        "    # tạo khoảng cách đầu và cuối câu\n",
        "    text_len = len(text) + 4\n",
        "    text = text.center(text_len)\n",
        "    \n",
        "    # dịch các từ tiếng anh thông dụng sang tiếng việt\n",
        "    for k, v in en2vi_dict.items():\n",
        "        key = ' '+k+' '\n",
        "        value = ' '+v+' '\n",
        "        text = text.replace(key, value)\n",
        "\n",
        "    # loại bỏ extra whilespace\n",
        "    text = re.sub(r'^\\s+$|\\s+$', ' ', text).strip()\n",
        "\n",
        "    # loại bỏ tab\n",
        "    text = re.sub(r'^\\s*|\\s\\s*', ' ', text).strip()\n",
        "\n",
        "    # tokenize\n",
        "    # rdrsegmenter = VnCoreNLP(\"/content/vncorenlp/VnCoreNLP-1.1.1.jar\", annotators=\"wseg\", max_heap_size='-Xmx500m')\n",
        "    # sentences = rdrsegmenter.tokenize(text) \n",
        "    # for sentence in sentences:\n",
        "    #     text = \" \".join(sentence)\n",
        "    # text = ViTokenizer.tokenize(text)\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "pWGjwehaSxAF"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['preprocess'] = df['content'].map(preprocessing_vietnamese)"
      ],
      "metadata": {
        "id": "jpSu5vKeaQXP"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Autocorrect word with difflib library and tokenize with pyvi"
      ],
      "metadata": {
        "id": "PG2eNPBrlDgs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import difflib\n",
        "\n",
        "def count_ngrams(series: pd.Series, n: int) -> pd.Series:\n",
        "    ngrams = series.copy().str.split(' ').explode()\n",
        "    for i in range(1, n):\n",
        "        ngrams += ' ' + ngrams.groupby(level=0).shift(-i)\n",
        "        ngrams = ngrams.dropna()\n",
        "    return ngrams.value_counts() \n",
        "\n",
        "top_percentile = 95     # what word covers 95%\n",
        "bi_grams = count_ngrams(df['preprocess'], n=2)\n",
        "df_bi_grams = pd.DataFrame(bi_grams)\n",
        "top_bi_grams = np.floor(np.percentile(df_bi_grams, top_percentile))\n",
        "\n",
        "word_top_bi_grams = df_bi_grams[df_bi_grams['preprocess'] >= top_bi_grams] # get top correct word with percentile 95% in dataset\n",
        "word_bottom_bi_grams = df_bi_grams[df_bi_grams['preprocess'] < top_bi_grams] # bottom word remaining\n",
        "\n",
        "list_word_top_bi_grams = word_top_bi_grams.index.tolist()\n",
        "list_word_bottom_bi_grams = word_bottom_bi_grams.index.tolist()\n",
        "\n",
        "correct_word_dict = {}\n",
        "for elem in list_word_bottom_bi_grams:\n",
        "    closest = difflib.get_close_matches(elem, list_word_top_bi_grams, cutoff=0.95) # map list top word to list bottom word with 95% probability\n",
        "    if closest:\n",
        "        correct_word_dict[elem] = closest[0]\n",
        "\n",
        "print(correct_word_dict)"
      ],
      "metadata": {
        "id": "aqKHAzxtaOKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def autocorrect_and_tokenize(text):\n",
        "\n",
        "    # tạo khoảng cách đầu và cuối câu\n",
        "    text_len = len(text) + 4\n",
        "    text = text.center(text_len)\n",
        "\n",
        "    # auto correct\n",
        "    for k, v in correct_word_dict.items():\n",
        "        key = ' '+k+' '\n",
        "        value = ' '+v+' '\n",
        "        text = text.replace(key, value)\n",
        "\n",
        "    # loại bỏ extra whilespace\n",
        "    text = re.sub(r'^\\s+$|\\s+$', ' ', text).strip()\n",
        "\n",
        "    # loại bỏ tab\n",
        "    text = re.sub(r'^\\s*|\\s\\s*', ' ', text).strip()\n",
        "\n",
        "    # tokenize\n",
        "    text = ViTokenizer.tokenize(text)\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "QyMvV_UmpDBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['preprocess'] = df['preprocess'].apply(autocorrect_and_tokenize)\n",
        "df"
      ],
      "metadata": {
        "id": "NoygJZEUnuTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gán nhãn dự liệu sentiment dựa vào rating\n",
        "sentiment = {1: 0,\n",
        "            2: 0,\n",
        "            3: 0,\n",
        "            4: 1,\n",
        "            5: 1}\n",
        "\n",
        "df[\"target\"] = df[\"score\"].map(sentiment)\n",
        "df = df[['preprocess','target']].dropna()\n",
        "df = df.query('preprocess != \"\"')\n",
        "\n",
        "\n",
        "df['num_word'] = [len(sentence.split()) for sentence in df['preprocess']]\n",
        "df"
      ],
      "metadata": {
        "id": "ZCqZg5rpS4kZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing train, test, val data"
      ],
      "metadata": {
        "id": "F8OrAomSTHY7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_df , remaining = train_test_split(df,\n",
        "                                        train_size=0.8,\n",
        "                                        random_state=42,\n",
        "                                        stratify=df.target.values)\n",
        "\n",
        "val_df, test_df = train_test_split(remaining,\n",
        "                                   train_size=0.5,\n",
        "                                   random_state=42,\n",
        "                                   stratify=remaining.target.values)\n",
        "\n",
        "len(train_df), len(val_df), len(test_df)"
      ],
      "metadata": {
        "id": "7Eq4eg59TG4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sentences = train_df[\"preprocess\"].tolist()\n",
        "val_sentences = val_df[\"preprocess\"].tolist()\n",
        "test_sentences = test_df[\"preprocess\"].tolist()"
      ],
      "metadata": {
        "id": "7Zy3OtkyTv4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting a base model"
      ],
      "metadata": {
        "id": "LFU0FgUlX_Z7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Base model Naive Bayes"
      ],
      "metadata": {
        "id": "LDQhlp4dYX2D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to evaluate: accuracy, precision, recall, f1-score\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "def calculate_results(y_true, y_pred):\n",
        "  \"\"\"\n",
        "  Calculates model accuracy, precision, recall and f1 score of a binary classification model.\n",
        "\n",
        "  Args:\n",
        "  -----\n",
        "  y_true = true labels in the form of a 1D array\n",
        "  y_pred = predicted labels in the form of a 1D array\n",
        "\n",
        "  Returns a dictionary of accuracy, precision, recall, f1-score.\n",
        "  \"\"\"\n",
        "  # Calculate model accuracy\n",
        "  model_accuracy = accuracy_score(y_true, y_pred) * 100\n",
        "  # Calculate model precision, recall and f1 score using \"weighted\" average\n",
        "  model_precision, model_recall, model_f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n",
        "  model_results = {\"accuracy\": model_accuracy,\n",
        "                  \"precision\": model_precision,\n",
        "                  \"recall\": model_recall,\n",
        "                  \"f1\": model_f1}\n",
        "  return model_results"
      ],
      "metadata": {
        "id": "xB2yOnlLYfXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Create a pipeline\n",
        "model_0 = Pipeline([\n",
        "  (\"tf-idf\", TfidfVectorizer()),\n",
        "  (\"clf\", MultinomialNB())\n",
        "])\n",
        "\n",
        "# Fit the pipeline to the training data\n",
        "model_0.fit(X=train_sentences, \n",
        "            y=train_df['target']);"
      ],
      "metadata": {
        "id": "zL5ovpfFYBIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_0_predict = model_0.predict(val_sentences)\n",
        "\n",
        "model_0_results = calculate_results(y_true=val_df['target'],\n",
        "                                     y_pred=model_0_predict)\n",
        "model_0_results"
      ],
      "metadata": {
        "id": "-UAyaAVjYosD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing data for deep learning model\n"
      ],
      "metadata": {
        "id": "uApDe9GGZWjY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "T7NPLrMHZaEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing text vectorizer and embedding for sentence level"
      ],
      "metadata": {
        "id": "G5Ljnwo2ICMu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = []\n",
        "\n",
        "for word in df['preprocess']:\n",
        "    words.append(word)\n",
        "\n",
        "max_tokens = len(set(words))"
      ],
      "metadata": {
        "id": "rZYkWUINIGH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create text vectorizer\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "\n",
        "text_vectorizer = TextVectorization(max_tokens=max_tokens, # number of words in vocabulary\n",
        "                                    standardize=\"lower\", \n",
        "                                    split=\"whitespace\",\n",
        "                                    output_sequence_length=128) # desired output length of vectorized sequences"
      ],
      "metadata": {
        "id": "5OZuWrpBIV1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adapt text vectorizer to training sentences\n",
        "text_vectorizer.adapt(train_sentences)"
      ],
      "metadata": {
        "id": "zWtf4hm6IoND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = text_vectorizer.get_vocabulary()\n",
        "len(vocab)"
      ],
      "metadata": {
        "id": "pdIIaGaNJ1P8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create token embedding layer\n",
        "token_embed = layers.Embedding(input_dim=len(vocab), # length of vocabulary\n",
        "                               output_dim=128, # Note: different embedding sizes result in drastically different numbers of parameters to train\n",
        "                               # Use masking to handle variable sequence lengths (save space)\n",
        "                               mask_zero=True,\n",
        "                               name=\"token_embedding\") "
      ],
      "metadata": {
        "id": "lgCp7CLZZMk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparing sentence"
      ],
      "metadata": {
        "id": "ER0_8P3GHJZh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# How long is each sentence on average?\n",
        "sent_lens = [len(sentence.split()) for sentence in train_sentences]\n",
        "avg_sent_len = np.mean(sent_lens)\n",
        "avg_sent_len # return average sentence length (in tokens)"
      ],
      "metadata": {
        "id": "OwwDiX14HIuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# What's the distribution look like?\n",
        "import matplotlib.pyplot as plt\n",
        "plt.hist(sent_lens, bins=10);"
      ],
      "metadata": {
        "id": "Mu7RpO5_HfI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How long of a sentence covers 95% of the lengths?\n",
        "output_seq_len = int(np.percentile(sent_lens, 95))\n",
        "output_seq_len"
      ],
      "metadata": {
        "id": "uvn8bEuXHnja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparing labels one hot"
      ],
      "metadata": {
        "id": "fTLPAtLiCh8O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "train_labels_one_hot = tf.one_hot(train_df['target'].to_numpy(), depth=len(set(df['target'])))\n",
        "val_labels_one_hot = tf.one_hot(val_df['target'].to_numpy(), depth=len(set(df['target'])))\n",
        "test_labels_one_hot = tf.one_hot(test_df['target'].to_numpy(), depth=len(set(df['target'])))\n",
        "\n",
        "train_labels_one_hot"
      ],
      "metadata": {
        "id": "S0y1q5_AB--4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn our data into TensorFlow Datasets\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_sentences, train_labels_one_hot))\n",
        "valid_dataset = tf.data.Dataset.from_tensor_slices((val_sentences, val_labels_one_hot))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_sentences, test_labels_one_hot))\n",
        "\n",
        "train_dataset"
      ],
      "metadata": {
        "id": "2UkBBQHfavAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take the TensorSliceDataset's and turn them into prefetched batches\n",
        "train_dataset = train_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "valid_dataset = valid_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "test_dataset = test_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "train_dataset"
      ],
      "metadata": {
        "id": "5BaDD0JWayt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model_1: Sentence level Conv1D"
      ],
      "metadata": {
        "id": "WOxhT9miNKx2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create 1D convolutional model to process sequences\n",
        "inputs = layers.Input(shape=(1,), dtype=tf.string)\n",
        "text_vectors = text_vectorizer(inputs) # vectorize text inputs\n",
        "token_embeddings = token_embed(text_vectors) # create embedding\n",
        "x = layers.Conv1D(64, kernel_size=5, padding=\"same\", activation=\"relu\")(token_embeddings)\n",
        "x = layers.MaxPooling1D(pool_size=2, strides=2, padding='same')(x)\n",
        "x = layers.Dropout(0.2)(x)\n",
        "x = layers.Conv1D(64, kernel_size=5, padding=\"same\", activation=\"relu\")(x)\n",
        "x = layers.MaxPooling1D(pool_size=2, strides=2, padding='same')(x)\n",
        "x = layers.Dropout(0.2)(x)\n",
        "x = layers.GlobalAveragePooling1D()(x) # condense the output of our feature vector\n",
        "x = layers.Dropout(0.2)(x)\n",
        "outputs = layers.Dense(len(set(df['target'])), activation=\"softmax\")(x)\n",
        "model_1 = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "# Compile\n",
        "model_1.compile(loss=\"categorical_crossentropy\", # if your labels are integer form (not one hot) use sparse_categorical_crossentropy\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=[\"accuracy\"])\n",
        "model_1.summary()"
      ],
      "metadata": {
        "id": "u62mrmSha0Es"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model\n",
        "model_1_history = model_1.fit(train_dataset,\n",
        "                              epochs=100,\n",
        "                              validation_data=valid_dataset,\n",
        "                              callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5)]) # only validate on 10% of batches\n",
        "\n",
        "model_1.evaluate(valid_dataset)"
      ],
      "metadata": {
        "id": "5zPO3-F4bBIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions (our model outputs prediction probabilities for each class)\n",
        "model_1_pred_probs = model_1.predict(test_dataset)\n",
        "\n",
        "# Convert pred probs to classes\n",
        "model_1_preds = tf.argmax(model_1_pred_probs, axis=1)\n",
        "\n",
        "# Calculate model_1 results\n",
        "model_1_results = calculate_results(y_true=test_df['target'],\n",
        "                                    y_pred=model_1_preds)\n",
        "model_1_results"
      ],
      "metadata": {
        "id": "eao0v5zUM0HI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model_2: Sentence level with LSTM model"
      ],
      "metadata": {
        "id": "gwDF4M7iNP04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = layers.Input(shape=(1,), dtype=tf.string)\n",
        "text_vectors = text_vectorizer(inputs) # vectorize text inputs\n",
        "token_embeddings = token_embed(text_vectors) # create embedding\n",
        "x = layers.Bidirectional(layers.LSTM(32, return_sequences=True))(token_embeddings)\n",
        "x = layers.LSTM(32)(x)\n",
        "x = layers.Dropout(0.4)(x)\n",
        "outputs = layers.Dense(len(set(df['target'])), activation=\"softmax\")(x)\n",
        "model_2 = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "# Compile\n",
        "model_2.compile(loss=\"categorical_crossentropy\", # if your labels are integer form (not one hot) use sparse_categorical_crossentropy\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=[\"accuracy\"])\n",
        "model_2.summary()"
      ],
      "metadata": {
        "id": "IDxZoK5Cf2uh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model\n",
        "model_2_history = model_2.fit(train_dataset,\n",
        "                              epochs=100,\n",
        "                              validation_data=valid_dataset,\n",
        "                              callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5)])\n",
        "model_2.evaluate(valid_dataset)"
      ],
      "metadata": {
        "id": "4HjAhMNgfxoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions (our model outputs prediction probabilities for each class)\n",
        "model_2_pred_probs = model_2.predict(test_dataset)\n",
        "\n",
        "# Convert pred probs to classes\n",
        "model_2_preds = tf.argmax(model_2_pred_probs, axis=1)\n",
        "\n",
        "# Calculate model_1 results\n",
        "model_2_results = calculate_results(y_true=test_df['target'],\n",
        "                                    y_pred=model_2_preds)\n",
        "model_2_results"
      ],
      "metadata": {
        "id": "EWir0lJTN_ii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model_3: Sentence level with transformer"
      ],
      "metadata": {
        "id": "P4R3a7tXiRca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = tf.keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)"
      ],
      "metadata": {
        "id": "RcWhiZgYiTi2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions"
      ],
      "metadata": {
        "id": "vAX8Gvu5ifp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(vocab)  # Only consider the top 20k words\n",
        "maxlen = 128  # Only consider the first 200 words of each movie review\n",
        "embed_dim = 32  # Embedding size for each token\n",
        "num_heads = 4  # Number of attention heads\n",
        "ff_dim = 16  # Hidden layer size in feed forward network inside transformer\n",
        "\n",
        "inputs = layers.Input(shape=(maxlen,))\n",
        "embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
        "x = embedding_layer(inputs)\n",
        "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "x = transformer_block(x)\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "x = layers.Dropout(0.4)(x)\n",
        "x = layers.Dense(128, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.4)(x)\n",
        "outputs = layers.Dense(len(set(df['target'])), activation=\"softmax\")(x)\n",
        "\n",
        "model_3 = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# Compile\n",
        "model_3.compile(loss=\"categorical_crossentropy\", # if your labels are integer form (not one hot) use sparse_categorical_crossentropy\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=[\"accuracy\"])\n",
        "model_3.summary()"
      ],
      "metadata": {
        "id": "0H3caeASim4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model\n",
        "model_3_history = model_3.fit(text_vectorizer(train_sentences),\n",
        "                              train_labels_one_hot,\n",
        "                              epochs=100,\n",
        "                              validation_data=(text_vectorizer(val_sentences), val_labels_one_hot),\n",
        "                              callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5)])\n",
        "model_3.evaluate(text_vectorizer(val_sentences), val_labels_one_hot)"
      ],
      "metadata": {
        "id": "4wMo5bZNk9_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions (our model outputs prediction probabilities for each class)\n",
        "model_3_pred_probs = model_3.predict(text_vectorizer(test_sentences))\n",
        "\n",
        "\n",
        "# Convert pred probs to classes\n",
        "model_3_preds = tf.argmax(model_3_pred_probs, axis=1)\n",
        "\n",
        "# Calculate model_1 results\n",
        "model_3_results = calculate_results(y_true=test_df['target'],\n",
        "                                    y_pred=model_3_preds)\n",
        "model_3_results"
      ],
      "metadata": {
        "id": "KkdIKwD8OeRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model_4: Chars level with Conv1D"
      ],
      "metadata": {
        "id": "bh6wQF2YsfRm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make function to split sentences into characters\n",
        "def split_chars(text):\n",
        "  return \" \".join(list(text))"
      ],
      "metadata": {
        "id": "991goo4Wshrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split sequence-level data splits into character-level data splits\n",
        "train_chars = [split_chars(sentence) for sentence in train_sentences]\n",
        "val_chars = [split_chars(sentence) for sentence in val_sentences]\n",
        "test_chars = [split_chars(sentence) for sentence in test_sentences]\n",
        "print(train_chars[0])"
      ],
      "metadata": {
        "id": "x2kr6J5dtH5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# What's the average character length?\n",
        "char_lens = [len(sentence) for sentence in train_sentences]\n",
        "mean_char_len = np.mean(char_lens)\n",
        "mean_char_len"
      ],
      "metadata": {
        "id": "2nSTxN8stKA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the distribution of our sequences at character-level\n",
        "import matplotlib.pyplot as plt\n",
        "plt.hist(char_lens, bins=7);"
      ],
      "metadata": {
        "id": "oi3bZ8GPtNrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find what character length covers 95% of sequences\n",
        "output_seq_char_len = int(np.percentile(char_lens, 95))\n",
        "output_seq_char_len"
      ],
      "metadata": {
        "id": "VOudXKoMtPev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create char-level token vectorizer instance\n",
        "char_vectorizer = TextVectorization(max_tokens=10000,  \n",
        "                                    output_sequence_length=output_seq_char_len,\n",
        "                                    standardize=\"lower_and_strip_punctuation\",\n",
        "                                    name=\"char_vectorizer\")\n",
        "\n",
        "# Adapt character vectorizer to training characters\n",
        "char_vectorizer.adapt(train_chars)"
      ],
      "metadata": {
        "id": "y5YZyIiQtTA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check character vocabulary characteristics\n",
        "char_vocab = char_vectorizer.get_vocabulary()\n",
        "print(f\"Number of different characters in character vocab: {len(char_vocab)}\")\n",
        "print(f\"5 most common characters: {char_vocab[:5]}\")\n",
        "print(f\"5 least common characters: {char_vocab[-5:]}\")"
      ],
      "metadata": {
        "id": "8Zc44TVktf23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Test out character vectorizer\n",
        "random_train_chars = random.choice(train_chars)\n",
        "print(f\"Charified text:\\n{random_train_chars}\")\n",
        "print(f\"\\nLength of chars: {len(random_train_chars.split())}\")\n",
        "vectorized_chars = char_vectorizer([random_train_chars])\n",
        "print(f\"\\nVectorized chars:\\n{vectorized_chars}\")\n",
        "print(f\"\\nLength of vectorized chars: {len(vectorized_chars[0])}\")\n",
        "\n",
        "# Create char embedding layer\n",
        "char_embed = layers.Embedding(input_dim=1000, # number of different characters\n",
        "                              output_dim=25, # embedding dimension of each character (same as Figure 1 in https://arxiv.org/pdf/1612.05251.pdf)\n",
        "                              mask_zero=False, # don't use masks (this messes up model_5 if set to True)\n",
        "                              name=\"char_embed\")\n",
        "\n",
        "# Test out character embedding layer\n",
        "print(f\"Charified text (before vectorization and embedding):\\n{random_train_chars}\\n\")\n",
        "char_embed_example = char_embed(char_vectorizer([random_train_chars]))\n",
        "print(f\"Embedded chars (after vectorization and embedding):\\n{char_embed_example}\\n\")\n",
        "print(f\"Character embedding shape: {char_embed_example.shape}\")"
      ],
      "metadata": {
        "id": "T-_KLE6pttAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make Conv1D on chars only\n",
        "inputs = layers.Input(shape=(1,), dtype=\"string\")\n",
        "char_vectors = char_vectorizer(inputs)\n",
        "char_embeddings = char_embed(char_vectors)\n",
        "x = layers.Conv1D(64, kernel_size=5, padding=\"same\", activation=\"relu\")(char_embeddings)\n",
        "x = layers.GlobalMaxPool1D()(x)\n",
        "outputs = layers.Dense(len(set(df['target'])), activation=\"softmax\")(x)\n",
        "model_4 = tf.keras.Model(inputs=inputs,\n",
        "                         outputs=outputs,\n",
        "                         name=\"model_3_conv1D_char_embedding\")\n",
        "\n",
        "# Compile model\n",
        "model_4.compile(loss=\"categorical_crossentropy\",\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=[\"accuracy\"])\n",
        "\n",
        "model_4.summary(0)"
      ],
      "metadata": {
        "id": "7enQXOomt5ge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create char datasets\n",
        "train_char_dataset = tf.data.Dataset.from_tensor_slices((train_chars, train_labels_one_hot)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "val_char_dataset = tf.data.Dataset.from_tensor_slices((val_chars, val_labels_one_hot)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "train_char_dataset"
      ],
      "metadata": {
        "id": "5hGfNiTVuFUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model\n",
        "model_4_history = model_4.fit(train_dataset,\n",
        "                              epochs=100,\n",
        "                              validation_data=valid_dataset,\n",
        "                              callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5)])\n",
        "model_4.evaluate(valid_dataset)"
      ],
      "metadata": {
        "id": "9IyJb_vmuOZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions (our model outputs prediction probabilities for each class)\n",
        "model_4_pred_probs = model_4.predict(test_dataset)\n",
        "\n",
        "# Convert pred probs to classes\n",
        "model_4_preds = tf.argmax(model_4_pred_probs, axis=1)\n",
        "\n",
        "# Calculate model_1 results\n",
        "model_4_results = calculate_results(y_true=test_df['target'],\n",
        "                                    y_pred=model_4_preds)\n",
        "model_4_results"
      ],
      "metadata": {
        "id": "Uk8NGWOkQJGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model_5: Combined sentences model and chars level"
      ],
      "metadata": {
        "id": "zJKX--wCutjL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Setup token inputs/model\n",
        "token_inputs = layers.Input(shape=(1,), dtype=tf.string, name=\"token_input\")\n",
        "token_embeddings = token_embed(text_vectorizer(token_inputs))\n",
        "x = layers.GlobalAveragePooling1D()(token_embeddings)\n",
        "x = layers.Dense(128, activation=\"relu\")(x)\n",
        "token_output = layers.Dropout(0.4)(x)\n",
        "token_model = tf.keras.Model(inputs=token_inputs,\n",
        "                             outputs=token_output)\n",
        "token_model.summary()\n",
        "# 2. Setup char inputs/model\n",
        "char_inputs = layers.Input(shape=(1,), dtype=tf.string, name=\"char_input\")\n",
        "char_vectors = char_vectorizer(char_inputs)\n",
        "char_embeddings = char_embed(char_vectors)\n",
        "x = layers.Bidirectional(layers.LSTM(16, return_sequences=True))(char_embeddings)\n",
        "x = layers.LSTM(16)(x)\n",
        "char_bi_lstm = layers.Dropout(0.4)(x) # bi-LSTM shown in Figure 1 of https://arxiv.org/pdf/1612.05251.pdf\n",
        "char_model = tf.keras.Model(inputs=char_inputs,\n",
        "                            outputs=char_bi_lstm)\n",
        "print('\\n')\n",
        "char_model.summary()\n",
        "# 3. Concatenate token and char inputs (create hybrid token embedding)\n",
        "token_char_concat = layers.Concatenate(name=\"token_char_hybrid\")([token_model.output, \n",
        "                                                                  char_model.output])\n",
        "\n",
        "# 4. Create output layers - addition of dropout discussed in 4.2 of https://arxiv.org/pdf/1612.05251.pdf\n",
        "combined_dropout = layers.Dropout(0.5)(token_char_concat)\n",
        "combined_dense = layers.Dense(200, activation=\"relu\")(combined_dropout) # slightly different to Figure 1 due to different shapes of token/char embedding layers\n",
        "final_dropout = layers.Dropout(0.5)(combined_dense)\n",
        "output_layer = layers.Dense(len(set(df['target'])), activation=\"softmax\")(final_dropout)\n",
        "\n",
        "# 5. Construct model with char and token inputs\n",
        "model_5 = tf.keras.Model(inputs=[token_model.input, char_model.input],\n",
        "                         outputs=output_layer,\n",
        "                         name=\"model_4_token_and_char_embeddings\")\n",
        "print('\\n')\n",
        "model_5.summary()"
      ],
      "metadata": {
        "id": "zTxAk-pAux_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot hybrid token and character model\n",
        "from tensorflow.keras.utils import plot_model\n",
        "plot_model(model_5)"
      ],
      "metadata": {
        "id": "PyV9I3_sv9aX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile token char model\n",
        "model_5.compile(loss=\"categorical_crossentropy\",\n",
        "                optimizer=tf.keras.optimizers.Adam(), # section 4.2 of https://arxiv.org/pdf/1612.05251.pdf mentions using SGD but we'll stick with Adam\n",
        "                metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "eY2stX02wFNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine chars and tokens into a dataset\n",
        "train_char_token_data = tf.data.Dataset.from_tensor_slices((train_sentences, train_chars)) # make data\n",
        "train_char_token_labels = tf.data.Dataset.from_tensor_slices(train_labels_one_hot) # make labels\n",
        "train_char_token_dataset = tf.data.Dataset.zip((train_char_token_data, train_char_token_labels)) # combine data and labels\n",
        "\n",
        "# Prefetch and batch train data\n",
        "train_char_token_dataset = train_char_token_dataset.batch(32).prefetch(tf.data.AUTOTUNE) \n",
        "\n",
        "# Repeat same steps validation data\n",
        "val_char_token_data = tf.data.Dataset.from_tensor_slices((val_sentences, val_chars))\n",
        "val_char_token_labels = tf.data.Dataset.from_tensor_slices(val_labels_one_hot)\n",
        "val_char_token_dataset = tf.data.Dataset.zip((val_char_token_data, val_char_token_labels))\n",
        "\n",
        "# Prefetch and batch validation data\n",
        "val_char_token_dataset = val_char_token_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Repeat same steps test data\n",
        "test_char_token_data = tf.data.Dataset.from_tensor_slices((test_sentences, test_chars))\n",
        "test_char_token_labels = tf.data.Dataset.from_tensor_slices(test_labels_one_hot)\n",
        "test_char_token_dataset = tf.data.Dataset.zip((test_char_token_data, test_char_token_labels))\n",
        "\n",
        "# Prefetch and batch validation data\n",
        "test_char_token_dataset = test_char_token_dataset.batch(32).prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "bBuAZgoLwKS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model on tokens and chars\n",
        "model_5_history = model_5.fit(train_char_token_dataset, # train on dataset of token and characters\n",
        "                              epochs=100,\n",
        "                              validation_data=val_char_token_dataset,\n",
        "                              callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5)])\n",
        "model_5.evaluate(val_char_token_dataset)"
      ],
      "metadata": {
        "id": "xwxrppGuwMmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions (our model outputs prediction probabilities for each class)\n",
        "model_5_pred_probs = model_5.predict(test_char_token_dataset)\n",
        "\n",
        "# Convert pred probs to classes\n",
        "model_5_preds = tf.argmax(model_5_pred_probs, axis=1)\n",
        "\n",
        "# Calculate model_1 results\n",
        "model_5_results = calculate_results(y_true=test_df['target'],\n",
        "                                    y_pred=model_5_preds)\n",
        "model_5_results"
      ],
      "metadata": {
        "id": "4vqWdIRpQWLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model_6: Combined token models + chars model + one_hot number of token "
      ],
      "metadata": {
        "id": "Q9giL4wT51qO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the coverage of a \"total_lines\" value of 20\n",
        "len_num = np.percentile(train_df.num_word, 98) # a value of 20 covers 98% of samples\n",
        "len_num"
      ],
      "metadata": {
        "id": "t5QVaTHD5-rz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_num = max(df['num_word'])"
      ],
      "metadata": {
        "id": "pUWEoCdA-ZE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use TensorFlow to create one-hot-encoded tensors of our \"total_lines\" column \n",
        "train_num_word_one_hot = tf.one_hot(train_df[\"num_word\"].to_numpy(), depth=max_num)\n",
        "val_num_word_one_hot = tf.one_hot(val_df[\"num_word\"].to_numpy(), depth=max_num)\n",
        "test_num_word_one_hot = tf.one_hot(test_df[\"num_word\"].to_numpy(), depth=max_num)\n",
        "\n",
        "# Check shape and samples of total lines one-hot tensor\n",
        "train_num_word_one_hot.shape, train_num_word_one_hot[:10]"
      ],
      "metadata": {
        "id": "U66LfypE6P1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Setup token inputs/model\n",
        "token_inputs = layers.Input(shape=(1,), dtype=tf.string, name=\"token_input\")\n",
        "token_embeddings = token_embed(text_vectorizer(token_inputs))\n",
        "x = layers.Conv1D(64, kernel_size=2, padding=\"same\", activation=\"relu\")(token_embeddings)\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "x = layers.Dropout(0.2)(x)\n",
        "x = layers.Dense(128, activation=\"relu\")(x)\n",
        "token_output = layers.Dropout(0.2)(x)\n",
        "token_model = tf.keras.Model(inputs=token_inputs,\n",
        "                             outputs=token_output)\n",
        "\n",
        "# 2. Setup char inputs/model\n",
        "char_inputs = layers.Input(shape=(1,), dtype=tf.string, name=\"char_input\")\n",
        "char_vectors = char_vectorizer(char_inputs)\n",
        "char_embeddings = char_embed(char_vectors)\n",
        "x = layers.Bidirectional(layers.LSTM(16, return_sequences=True))(char_embeddings)\n",
        "x = layers.LSTM(16)(x)\n",
        "char_bi_lstm = layers.Dropout(0.2)(x) # bi-LSTM shown in Figure 1 of https://arxiv.org/pdf/1612.05251.pdf\n",
        "char_model = tf.keras.Model(inputs=char_inputs,\n",
        "                            outputs=char_bi_lstm)\n",
        "\n",
        "# 3. Setup num word inputs/model\n",
        "num_word_inputs = layers.Input(shape=(max_num,), dtype=tf.int32, name=\"total_lines_input\")\n",
        "y = layers.Dense(128, activation=\"relu\")(num_word_inputs)\n",
        "y = layers.Dropout(0.2)(y)\n",
        "num_word_model = tf.keras.Model(inputs=num_word_inputs,\n",
        "                                  outputs=y)\n",
        "\n",
        "\n",
        "\n",
        "# 4. Concatenate token and char inputs (create hybrid token embedding)\n",
        "token_char_concat = layers.Concatenate(name=\"token_char_hybrid\")([token_model.output, \n",
        "                                                                  char_model.output])\n",
        "\n",
        "z = layers.Dense(128, activation=\"relu\")(token_char_concat)\n",
        "z = layers.Dropout(0.2)(z)\n",
        "\n",
        "\n",
        "# 4. Create output layers - addition of dropout discussed in 4.2 of https://arxiv.org/pdf/1612.05251.pdf\n",
        "z = layers.Concatenate(name=\"token_char_positional_embedding\")([num_word_model.output,\n",
        "                                                                z])\n",
        "\n",
        "# 7. Create output layer\n",
        "output_layer = layers.Dense(len(set(df['target'])), activation=\"softmax\", name=\"output_layer\")(z)\n",
        "\n",
        "# 8. Put together model\n",
        "model_6 = tf.keras.Model(inputs=[num_word_model.input,\n",
        "                                 token_model.input, \n",
        "                                 char_model.input],\n",
        "                         outputs=output_layer)\n",
        "\n",
        "model_6.summary()"
      ],
      "metadata": {
        "id": "pYuqTYYk6qF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Plot hybrid token and character model\n",
        "# import matplotlib.pyplot as plt\n",
        "# from tensorflow.keras.utils import plot_model\n",
        "\n",
        "# plot_model(model_6)"
      ],
      "metadata": {
        "id": "wL0YfzOS7-X4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile token char model\n",
        "model_6.compile(loss=\"categorical_crossentropy\",\n",
        "                optimizer=tf.keras.optimizers.Adam(), # section 4.2 of https://arxiv.org/pdf/1612.05251.pdf mentions using SGD but we'll stick with Adam\n",
        "                metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "mvt5b7Oj8Jly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create training and validation datasets (all four kinds of inputs)\n",
        "train_pos_char_token_data = tf.data.Dataset.from_tensor_slices((train_num_word_one_hot, # total lines\n",
        "                                                                train_sentences, # train tokens\n",
        "                                                                train_chars)) # train chars\n",
        "train_pos_char_token_labels = tf.data.Dataset.from_tensor_slices(train_labels_one_hot) # train labels\n",
        "train_pos_char_token_dataset = tf.data.Dataset.zip((train_pos_char_token_data, train_pos_char_token_labels)) # combine data and labels\n",
        "train_pos_char_token_dataset = train_pos_char_token_dataset.batch(32).prefetch(tf.data.AUTOTUNE) # turn into batches and prefetch appropriately\n",
        "\n",
        "# Validation dataset\n",
        "val_pos_char_token_data = tf.data.Dataset.from_tensor_slices((val_num_word_one_hot,\n",
        "                                                              val_sentences,\n",
        "                                                              val_chars))\n",
        "val_pos_char_token_labels = tf.data.Dataset.from_tensor_slices(val_labels_one_hot)\n",
        "val_pos_char_token_dataset = tf.data.Dataset.zip((val_pos_char_token_data, val_pos_char_token_labels))\n",
        "val_pos_char_token_dataset = val_pos_char_token_dataset.batch(32).prefetch(tf.data.AUTOTUNE) # turn into batches and prefetch appropriately\n",
        "\n",
        "# Test dataset\n",
        "test_pos_char_token_data = tf.data.Dataset.from_tensor_slices((test_num_word_one_hot,\n",
        "                                                              test_sentences,\n",
        "                                                              test_chars))\n",
        "test_pos_char_token_labels = tf.data.Dataset.from_tensor_slices(test_labels_one_hot)\n",
        "test_pos_char_token_dataset = tf.data.Dataset.zip((test_pos_char_token_data, test_pos_char_token_labels))\n",
        "test_pos_char_token_dataset = test_pos_char_token_dataset.batch(32).prefetch(tf.data.AUTOTUNE) # turn into batches and prefetch appropriately\n",
        "\n",
        "# Check input shapes\n",
        "train_pos_char_token_dataset, val_pos_char_token_dataset, test_pos_char_token_dataset"
      ],
      "metadata": {
        "id": "kuHl4qyl8svr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the token, char and positional embedding model\n",
        "history_model_6 = model_6.fit(train_pos_char_token_dataset,\n",
        "                              epochs=100,\n",
        "                              validation_data=val_pos_char_token_dataset,\n",
        "                              callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5)])\n",
        "model_6.evaluate(test_pos_char_token_dataset)"
      ],
      "metadata": {
        "id": "xIbepjRi85Iq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions (our model outputs prediction probabilities for each class)\n",
        "model_6_pred_probs = model_6.predict(test_pos_char_token_dataset)\n",
        "\n",
        "# Convert pred probs to classes\n",
        "model_6_preds = tf.argmax(model_6_pred_probs, axis=1)\n",
        "\n",
        "# Calculate model_1 results\n",
        "model_6_results = calculate_results(y_true=test_df['target'],\n",
        "                                    y_pred=model_6_preds)\n",
        "model_6_results"
      ],
      "metadata": {
        "id": "tRNxby2QQ_EY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compare results"
      ],
      "metadata": {
        "id": "DJys3MxGSTkl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine model results into a DataFrame\n",
        "all_model_results = pd.DataFrame({\"baseline\": model_0_results,\n",
        "                                  \"token level Conv1D\": model_1_results,\n",
        "                                  \"token level with LSTM\": model_2_results,\n",
        "                                  \"token level with transformer\": model_3_results,\n",
        "                                  \"chars level with Conv1D\": model_4_results,\n",
        "                                  \"combined token level and chars level\": model_5_results,\n",
        "                                  \"combined token level, chars level and number token in one_hot\": model_6_results})\n",
        "all_model_results = all_model_results.transpose()\n",
        "all_model_results"
      ],
      "metadata": {
        "id": "XNNxN6sCRfgY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}